{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import/Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=reset\n",
    "#step_name=reset\n",
    "#step_type=import/export\n",
    "#step_desc=reload the original data frame from the csv file.\n",
    "df=pd.read_csv(\"/Users/raafat.hantoush/Documents/GitHub/general/work_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=import_required_libs\n",
    "#step_name=import required libraries\n",
    "#step_type=import/export\n",
    "#step_desc=import the required python libraries\n",
    "import pandas as pd\n",
    "import numpy as np;\n",
    "import scipy;\n",
    "## plotting libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "## stats Libraries\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "## Sklearn libraries\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=export_data_to_csv\n",
    "#step_name=export data to csv\n",
    "#step_type=import/export\n",
    "#step_desc=export pandas data frame to csv file\n",
    "df.to_csv ('', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=custom_code\n",
    "#step_name=custom code\n",
    "#step_type=import/export\n",
    "#step_desc= custom code\n",
    "# Your Custom Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=standardize_columns\n",
    "#step_name=standardize columns\n",
    "#step_type=data_cleaning\n",
    "#step_desc=remove spaces from data frame columns\n",
    "df.columns= df.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=lower_cols\n",
    "#step_name=lower columns\n",
    "#step_type=data_cleaning\n",
    "#step_desc=make the data frame columns a lower case.\n",
    "df.columns= df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=rename_cols\n",
    "#step_name=rename columns\n",
    "#step_type=data_cleaning\n",
    "#step_desc=renaming columns\n",
    "df = df.rename(columns=input_mapper)  \n",
    "# input_mapper -> {\"old_name\": \"new_name\", ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=drop_duplicates\n",
    "#step_name=drop duplicates\n",
    "#step_type=data_cleaning\n",
    "#step_desc=removing duplicates (entire row)\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=drop_cols_high_perc_missing\n",
    "#step_name=drop high % of missing values\n",
    "#step_type=data_cleaning\n",
    "#step_desc=dropping column/ columns with high percentage of missing values\n",
    "def drop_columns_high_perc_missing(df, input_threshold=0.8):  # input_threshold -> float\n",
    "    column_list = []\n",
    "\n",
    "    for column in df.columns:\n",
    "\n",
    "        nan_ratio = df[column].isna().sum() / len(df[column])\n",
    "\n",
    "        if nan_ratio >= input_threshold:\n",
    "\n",
    "            column_list.append(column)\n",
    "\n",
    "    return df.drop(columns=column_list,inplace=True)\n",
    "\n",
    "# calling the function\n",
    "drop_columns_high_perc_missing(df, input_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=convert_cols_type\n",
    "#step_name=convert data type\n",
    "#step_type=data_cleaning\n",
    "#step_desc=correcting data type ( object to numeric, float to int, numeric to object)\n",
    "#input_mapper -> dict = {\"col\": \"data_type\", ...}\n",
    "df = df.astype(input_mapper) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=filter_rows_by_cond\n",
    "#step_name=filter pandas rows by condition\n",
    "#step_type=data_cleaning\n",
    "#step_desc=filter rows based on condition\n",
    "# column -> string / input_condition -> int, string, datetime, etc..\n",
    "df = df[df[column] == input_condition]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=filter_rows_by_index\n",
    "#step_name=filter pandas rows by index\n",
    "#step_type=data_cleaning\n",
    "#step_desc=filter rows by zero-based index  (iloc)\n",
    "# input_n0 -> int, left index of the slicing / input_nf -> int, right index of the slicing\n",
    "df = df.iloc[input_n0:input_nf, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=set_cols_values_by_cond\n",
    "#step_name=update pandas values\n",
    "#step_type=data_cleaning\n",
    "#step_desc=update pandas column specific values based on condition.\n",
    "# column -> string / input_condition -> int, string, datetime, etc. / input_value -> int, string, datetime, etc.\n",
    "df = df[df[column] == input_condition] = input_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=fill_missing_vals\n",
    "#step_name=filling missing values\n",
    "#step_type=data_cleaning\n",
    "#step_desc=filling missing values\n",
    "def fill_missing_vals(df, mapper_input):\n",
    "    for strategy, column_list in mapper_input.items():\n",
    "\n",
    "        imp_mean = SimpleImputer(missing_values=np.nan, strategy=strategy)  # the sklearn SimpleImputer is created\n",
    "        imp_mean.fit(df[column_list])  # the SimpleImputer is fitted using the target columns\n",
    "        df_target_columns_filled = imp_mean.transform(df[column_list])  # the target columns are transformed, i.e. nan values are filled\n",
    "    \n",
    "        df[column_list] = df_target_columns_filled  # the target columns of the main df are replaced by the filled ones\n",
    "\n",
    "    return df\n",
    "## calling the function\n",
    "fill_missing_vals(df, mapper_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=remove_outliers\n",
    "#step_name=remove outliers\n",
    "#step_type=data_cleaning\n",
    "#step_desc=Removing outliers\n",
    "from scipy.stats import scoreatpercentile as pct\n",
    "def remove_outliers(df):\n",
    "    pct_75 = pct(df, 75)  # Calculate percentile 75 using scipy function scoreatpercentile\n",
    "    pct_25 = pct(df, 25)  # Calculate percentile 25 using scipy function scoreatpercentile\n",
    "    upper_bound = pct_75 + 1.5*iqr(df)  # iqr - > Scipy function to calculate the Interquartile Range\n",
    "    lower_bound = pct_25 - 1.5*iqr(df)\n",
    "    df = df[(df <= upper_bound) & (df >= lower_bound)]  # Filter out the outliers\n",
    "    return df\n",
    "\n",
    "#calling the function\n",
    "remove_outliers(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=drop_cols\n",
    "#step_name=drop columns\n",
    "#step_type=feature_selection\n",
    "#step_desc=drop one or more columns from the data frame.\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=drop_high_corr_cols\n",
    "#step_name=drop highly corr cols\n",
    "#step_type=feature_selection\n",
    "#step_desc=dropping highly correlated columns\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=filter_by_P-value\n",
    "#step_name=filter by P-value\n",
    "#step_type=feature_selection\n",
    "#step_desc=filter by P-value\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=RFE\n",
    "#step_name=recursive feature elemination\n",
    "#step_type=feature_selection\n",
    "#step_desc=recursive feature elemination\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=add_computed_cols\n",
    "#step_name=add computed features\n",
    "#step_type=feature_engineering\n",
    "#step_desc=adding computed features\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=bucket_cols\n",
    "#step_name=bucket/bin features\n",
    "#step_type=feature_engineering\n",
    "#step_desc= discretize the column into equal-sized bins and assigning them specific labels\n",
    "labels=[1,2,3]\n",
    "pd.cut(df[\"col-name\"], 3,labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=dummy_variables\n",
    "#step_name=create dummy variables\n",
    "#step_type=feature_transformation\n",
    "#step_desc=Convert categorical variable into dummy/indicator variables.\n",
    "pd.get_dummies(df, prefix=['col1', 'col2'], columns=[column1,column2],drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=apply_transformer\n",
    "#step_name=apply Transformer\n",
    "#step_type=feature_transformation\n",
    "#step_desc=applying transformer to transform the data. Options are StandardScaler, MinMaxScaler, PowerTransformer and QuantileTransformer\n",
    "from sklearn import preprocessing\n",
    "def transform_data(data,method=\"StandardScaler\"):\n",
    "    if method == \"StandardScaler\":\n",
    "        transformer = preprocessing.StandardScaler()\n",
    "    elif method == \"MinMaxScaler\":\n",
    "        transformer = preprocessing.MinMaxScaler()\n",
    "    elif method == \"PowerTransformer\":\n",
    "        transformer = preprocessing.PowerTransformer()\n",
    "    elif method == \"QuantileTransformer\":\n",
    "        transformer= preprocessing.QuantileTransformer(random_state=0)\n",
    "    else: return \"No Trnasformation method is applied!\";\n",
    "\n",
    "    transformer = transformer.fit(data)\n",
    "    data=transformer.transform(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# transforming the data: data normally is X_train or X_test \n",
    "# or the whole numerical data as df._get_numeric_data()\n",
    "data=transform_data(data,method=\"StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=sample_data\n",
    "#step_name=take random sample\n",
    "#step_type=data_sampling\n",
    "#step_desc=take random sample\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=XY_split\n",
    "#step_name=XY split\n",
    "#step_type=data_splitting\n",
    "#step_desc=XY split: splitting the tagret variable Y from the independent features.\n",
    "X=df.drop('target_col', axis=1)\n",
    "y=df[[\"target_col\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=numercials_cateogricals_split\n",
    "#step_name=numercials cateogricals split\n",
    "#step_type=data_splitting\n",
    "#step_desc=splitting the data into numerical and categorical features\n",
    "numericals_df=X._get_numeric_data()\n",
    "categoricals_df= X.select_dtypes(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=train_test_split\n",
    "#step_name=train test split\n",
    "#step_type=data_splitting\n",
    "#step_desc=splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test=model_selection.train_test_split(X, y, test_size=.20,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=linear_regression\n",
    "#step_name=Linear Regression\n",
    "#step_type=Regression\n",
    "#step_desc=Linear Regression\n",
    "def apply_regression_model(X_train,y_train,X_test,y_test,model_name=\"LinearRegression\"):\n",
    "    if model_name == \"LinearRegression\":\n",
    "        model= LinearRegression()\n",
    "    elif model_name ==\"Lasso\":\n",
    "        model= Lasso()\n",
    "    elif model_name ==\"Ridge\":\n",
    "        model = Ridge()\n",
    "    elif model_name == \"ElasticNet\":\n",
    "        model=ElasticNet()\n",
    "    else: pass\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    y_pred_train=model.predict(X_train)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=statsmodel_linear_regression\n",
    "#step_name=Statsmodel Linear Regression\n",
    "#step_type=Regression\n",
    "#step_desc=OLS using statsmodel\n",
    "X_train_const= sm.add_constant(X_train) # adding a constant\n",
    "\n",
    "model = sm.OLS(y_train, X_train_const).fit()\n",
    "predictions_train = model.predict(X_train_const) \n",
    "\n",
    "X_test_const = sm.add_constant(X_test) # adding a constant\n",
    "predictions_test = model.predict(X_test_const) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=logisitc_regression\n",
    "#step_name=Logisitc Regression\n",
    "#step_type=Classification\n",
    "#step_desc=Logisitc Regression\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=kmeans_clustering\n",
    "#step_name=K-means\n",
    "#step_type=Clustering\n",
    "#step_desc=K-means\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=regression_model_evaluating\n",
    "#step_name=Regression Model Metrics\n",
    "#step_type=model_validation\n",
    "#step_desc=Regression Model Metrics\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
