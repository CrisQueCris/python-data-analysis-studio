{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import/Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=reset\n",
    "#step_name=reset\n",
    "#step_type=import/export\n",
    "#step_desc=reload the original data frame from the csv file.\n",
    "df=pd.read_csv(\"/Users/raafat.hantoush/Documents/GitHub/general/work_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=import_required_libs\n",
    "#step_name=import required libraries\n",
    "#step_type=import/export\n",
    "#step_desc=import the required python libraries\n",
    "import pandas as pd\n",
    "import numpy as np;\n",
    "import scipy;\n",
    "## plotting libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=export_data_to_csv\n",
    "#step_name=export data to csv\n",
    "#step_type=import/export\n",
    "#step_desc=export pandas data frame to csv file\n",
    "df.to_csv ('', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=custom_code\n",
    "#step_name=custom code\n",
    "#step_type=import/export\n",
    "#step_desc= custom code\n",
    "# Your Custom Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=standardize_columns\n",
    "#step_name=standardize columns\n",
    "#step_type=data_cleaning\n",
    "#step_desc=remove spaces from data frame columns\n",
    "df.columns= df.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=lower_cols\n",
    "#step_name=lower columns\n",
    "#step_type=data_cleaning\n",
    "#step_desc=make the data frame columns a lower case.\n",
    "df.columns= df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=rename_cols\n",
    "#step_name=rename columns\n",
    "#step_type=data_cleaning\n",
    "#step_desc=renaming columns\n",
    "df = df.rename(columns=input_mappertest)  \n",
    "# input_mapper -> {\"old_name\": \"new_name\", ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=drop_duplicates\n",
    "#step_name=drop duplicates\n",
    "#step_type=data_cleaning\n",
    "#step_desc=removing duplicates (entire row)\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=drop_cols_high_perc_missing\n",
    "#step_name=drop high % of missing values\n",
    "#step_type=data_cleaning\n",
    "#step_desc=dropping column/ columns with high percentage of missing values\n",
    "def drop_columns_high_perc_missing(df, input_threshold=0.8):  # input_threshold -> float\n",
    "    column_list = []\n",
    "\n",
    "    for column in df.columns:\n",
    "\n",
    "        nan_ratio = df[column].isna().sum() / len(df[column])\n",
    "\n",
    "        if nan_ratio >= input_threshold:\n",
    "\n",
    "            column_list.append(column)\n",
    "\n",
    "    return df.drop(columns=column_list,inplace=True)\n",
    "\n",
    "# calling the function\n",
    "drop_columns_high_perc_missing(df, input_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=convert_cols_type\n",
    "#step_name=convert data type\n",
    "#step_type=data_cleaning\n",
    "#step_desc=correcting data type ( object to numeric, float to int, numeric to object)\n",
    "#input_mapper -> dict = {\"col\": \"data_type\", ...}\n",
    "df = df.astype(input_mapper) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=filter_rows_by_cond\n",
    "#step_name=filter pandas rows by condition\n",
    "#step_type=data_cleaning\n",
    "#step_desc=filter rows based on condition\n",
    "# column -> string / input_condition -> int, string, datetime, etc..\n",
    "df = df[df[column] == input_condition]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=filter_rows_by_index\n",
    "#step_name=filter pandas rows by index\n",
    "#step_type=data_cleaning\n",
    "#step_desc=filter rows by zero-based index  (iloc)\n",
    "# input_n0 -> int, left index of the slicing / input_nf -> int, right index of the slicing\n",
    "df = df.iloc[input_n0:input_nf, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=set_cols_values_by_cond\n",
    "#step_name=update pandas values\n",
    "#step_type=data_cleaning\n",
    "#step_desc=update pandas column specific values based on condition.\n",
    "# column -> string / input_condition -> int, string, datetime, etc. / input_value -> int, string, datetime, etc.\n",
    "df = df[df[column] == input_condition] = input_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=fill_missing_vals\n",
    "#step_name=filling missing values\n",
    "#step_type=data_cleaning\n",
    "#step_desc=filling missing values\n",
    "def fill_missing_vals(df, mapper_input):\n",
    "    for strategy, column_list in mapper_input.items():\n",
    "\n",
    "        imp_mean = SimpleImputer(missing_values=np.nan, strategy=strategy)  # the sklearn SimpleImputer is created\n",
    "        imp_mean.fit(df[column_list])  # the SimpleImputer is fitted using the target columns\n",
    "        df_target_columns_filled = imp_mean.transform(df[column_list])  # the target columns are transformed, i.e. nan values are filled\n",
    "    \n",
    "        df[column_list] = df_target_columns_filled  # the target columns of the main df are replaced by the filled ones\n",
    "\n",
    "    return df\n",
    "## calling the function\n",
    "fill_missing_vals(df, mapper_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=remove_outliers\n",
    "#step_name=remove outliers\n",
    "#step_type=data_cleaning\n",
    "#step_desc=Removing outliers\n",
    "from scipy.stats import scoreatpercentile as pct\n",
    "def remove_outliers(df):\n",
    "    pct_75 = pct(df, 75)  # Calculate percentile 75 using scipy function scoreatpercentile\n",
    "    pct_25 = pct(df, 25)  # Calculate percentile 25 using scipy function scoreatpercentile\n",
    "    upper_bound = pct_75 + 1.5*iqr(df)  # iqr - > Scipy function to calculate the Interquartile Range\n",
    "    lower_bound = pct_25 - 1.5*iqr(df)\n",
    "    df = df[(df <= upper_bound) & (df >= lower_bound)]  # Filter out the outliers\n",
    "    return df\n",
    "\n",
    "#calling the function\n",
    "remove_outliers(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=drop_cols\n",
    "#step_name=drop columns\n",
    "#step_type=feature_selection\n",
    "#step_desc=drop one or more columns from the data frame.\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=drop_high_corr_cols\n",
    "#step_name=drop highly corr cols\n",
    "#step_type=feature_selection\n",
    "#step_desc=dropping highly correlated columns\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=filter_by_P-value\n",
    "#step_name=filter by P-value\n",
    "#step_type=feature_selection\n",
    "#step_desc=filter by P-value\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=RFE\n",
    "#step_name=recursive feature elemination\n",
    "#step_type=feature_selection\n",
    "#step_desc=recursive feature elemination\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=add_computed_cols\n",
    "#step_name=add computed features\n",
    "#step_type=feature_engineering\n",
    "#step_desc=adding computed features\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=bucket_cols\n",
    "#step_name=bucket/bin features\n",
    "#step_type=feature_engineering\n",
    "#step_desc=bucket/bin features\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=dummy_variables\n",
    "#step_name=create dummy variables\n",
    "#step_type=feature_transformation\n",
    "#step_desc=create dummy variables\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=sample_data\n",
    "#step_name=take random sample\n",
    "#step_type=data_sampling\n",
    "#step_desc=take random sample\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=XY_split\n",
    "#step_name=XY split\n",
    "#step_type=data_splitting\n",
    "#step_desc=XY split\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=linear_regression\n",
    "#step_name=Linear Regression\n",
    "#step_type=Regression\n",
    "#step_desc=Linear Regression\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=logisitc_regression\n",
    "#step_name=Logisitc Regression\n",
    "#step_type=Classification\n",
    "#step_desc=Logisitc Regression\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=kmeans_clustering\n",
    "#step_name=K-means\n",
    "#step_type=Clustering\n",
    "#step_desc=K-means\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_id=regression_model_evaluating\n",
    "#step_name=Regression Model Metrics\n",
    "#step_type=model_validation\n",
    "#step_desc=Regression Model Metrics\n",
    "pd.DataFrame.drop(df,columns=[\"\"],inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
